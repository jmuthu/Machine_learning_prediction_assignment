---
title: "Prediction of exercise quality using models trained with accelerometers data"
author: "Joe"
date: "30th Janurary 2015"
output: html_document
---
## Executive Summary
This report proposes a model to predict the quality of exercise based on data gathered by wearable accelerometers. Different models are trained using data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.  More information is available at [here](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset). The best model is chosen based on the prediction accuracy obtained on the cross validation dataset.

The training dataset has been preprocessed and cleaned of irrevelant/duplicate predictors. Highly correlated predictors have been eliminated. Predictors (excluding output variable) have been reduced from 159 variables to 34 variables after preprocessing. 

As the prediction is a multi classification problem, the models chosen were random forest and boosting which perform well on multi classification. The data was split into training and test set. Models were trained on training data with K fold cross validation and finally the best model was chosen on the prediction on the cross validation set. 

The model that performed the best was randomforest and it had an accuracy of 0.9988 and Kappa of 0.9985 on independent testing dataset.

Git repository for the above analysis is at https://github.com/jmuthu/Machine_learning_prediction_assignment

## Exploratory Analysis
Lets look at the training and test datasets, check the total count and also see if all columns are same in both data sets.

```{r, cache=TRUE, echo=FALSE,message=FALSE}
library(caret)
set.seed(32343)
data <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
tr <- names(data)
te <- names(testing)
data_summ <- data.frame("Details"= "Row count", "Training" = nrow(data), "Testing" = nrow(testing), stringsAsFactors=FALSE)
data_summ <- rbind(data_summ, c("Missing columns", te[which(tr != te)], tr[which(tr != te)]))
data_summ
```
The sample size of training is around 19622 which is a good medium size dataset. In the test set, the output column "classe" is missing and it has an additional index column "problem_id" which is missing in the training set. We can ignore that "problem_id" column as it is just an index and "classe" variable needs to be predicted for the test set. Except for these two columns all the other columns are same in test and training set.

Lets look at some the columns in the training set. There are totally `r length(tr)` variables in the training dataset
```{r, cache=TRUE, echo=FALSE,message=FALSE}
head(tr,20)
```
There are few time measurements, user name, index and rest are all measurements from the four devices namely roll belt, arm, dumbell and forearm. 

## Preprocessing
Lets do some preprocessing and reduce the number of variables which are irrevelant or duplicate or empty. 

#### Duplicate/Irrevelant predictors
The index column "X" is not relevant. The column "raw_timestamp_part_1" looks like a timestamp value for the date column "cvtd_timestamp". 
Lets convert the timestamp value to date and compare them. Lets check if any of these mismatch. 
```{r, cache=TRUE,message=FALSE}
sum(format(as.POSIXct(data$raw_timestamp_part_1, origin="1970-01-01", tz="UTC"),"%d/%m/%Y %H:%M") != data$cvtd_timestamp)
```
The mismatch count is zero so both time fields are same. Lets remove the X and cvtd_timestamp columns. 
```{r, cache=TRUE, echo=FALSE,message=FALSE}
data <- data[,c(-1,-5)]
```

####  Near Zero variablity predictor 

```{r, cache=TRUE, echo=FALSE,message=FALSE}
# Remove columns that have no or near zero variability
nzv <- nearZeroVar(data)
data <- data[,-nzv]
missing <- round(mean(is.na(data))*100,2)
```
Lets find those predictors using nearZeroVar funciton and remove it. About `r length(nzv)` predictors have no variablity at all. 

#### Missing values 
Lets remove predictors or/and rows that have missing values. There are about `r missing`% missing values in the training dataset. Lets see the distribution of missing value count for each predictor
```{r, cache=TRUE, echo=FALSE,message=FALSE}
# Checking for missing data

data_na_count <- sapply(data, function(y) sum(is.na(y) | y == ""))
as.data.frame(table(data_na_count))
data <- data[,names(subset(data_na_count,data_na_count==0))]
```
There are 41 predictors that have a huge count of missing values (19216). Lets remove these predictors from the dataset.

#### Highly Correlated Predictors 
Lets remove the highly correlated predictors from the data set which have correlation index of greater than 0.75. First we need to convert the character predictors "user_name" and "classe" to numeric to compute correlation.

Summary of the correlation distribution in the training set is show below. 

```{r, cache=TRUE, echo=FALSE,message=FALSE}
# Convert character columns to numeric
data$user_name <- factor(data$user_name)
user_levels <- levels(data$user_name)
data$user_name <- as.numeric(data$user_name)
testing$user_name <- as.numeric(factor(testing$user_name, levels=user_levels))

data$classe <- factor(data$classe)
classe_levels <- levels(data$classe)
data$classe <- as.numeric(data$classe)

# correlation 
data_cor <- cor(data)
summary(data_cor[upper.tri(data_cor)])
highlyCor <- findCorrelation(data_cor, cutoff = .75)
data_complete <- data[,-highlyCor]
data_complete$classe <- factor(data_complete$classe)
```

There were `r length(highlyCor)` highly correlated predictors and these can be removed as well. 

Now we have brought down the predictors from `r length(tr)` in the original dataset to `r length(colnames(data_complete))`. 
The data set looks clean with no missing values and all irrevelant/duplicate predictors have been removed. 

## Model Selection
The output variable classe has five different exercise categories of which A is correct execution and the rest (B,C,D,E) are all incorrect executions. As this is a multi classification problem, we will choose the following models as they work well for classification problems.

* Support Vector Machine with Radial Basis Function 
* Stochastic Gradient Boosting
* Random Forest

The best performing model will be chosen based on 10 fold cross validation on the training data. 
We will split (25%) of the training set into a separate testing data which will be used with the best model once to calculate final accuracy. 
As these models take time, we will leverage the parallel processing functionality from caret package. 
Lets train the model with the same 10 cross validation test.

```{r, cache=TRUE,echo=FALSE,message=FALSE}
# Split data into training and testing set. 
train_index <- createDataPartition(data_complete$classe,p=0.75, list=FALSE)
training <- data_complete[train_index,]
validation <- data_complete[-train_index,]

# Apply Models
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 2) # convention to leave 1 cor e for OS
registerDoParallel(cluster)
fitControl <- trainControl(method="cv", number=10,  allowParallel = TRUE)
model = readRDS("rfmodel.rds")
if (is.null(model)) {
        tm <- proc.time()
        set.seed(32343)
        model <- train(classe~., data=training, method="rf", preProc = c("center", "scale"), trControl = fitControl)
        rftime <- proc.time() - tm
}

gbmModel = readRDS("gbmModel.rds")
if (is.null(gbmModel)) {
        tm <- proc.time()        
        set.seed(32343)
        gbmModel <- train(classe~., data=training, method="gbm", preProc = c("center", "scale"), trControl = fitControl)
        gbmtime <- proc.time()-tm
}
svmModel = readRDS("svmModel.rds")
if (is.null(gbmModel)) {
        tm <- proc.time()     
        set.seed(32343)
        svmModel <- svmModel <- train(classe~., method="svmRadial", preProc = c("center", "scale"),data=training, trControl=fitControl)
        svmtime <- proc.time()-tm
}
```
Lets check the in sample error for these models and for that we will use the accuracy and kappa metrics. Comparison is shown below for these 3 models based on the performance on the cross validation set. 
```{r, cache=TRUE,echo=FALSE,message=FALSE}
results <- resamples(list(RF=model, GBM=gbmModel, SVM=svmModel))
# summarize the distributions
summary(results)
# boxplots of results
bwplot(results)
```
If you compare the accuracy and kappa means above, Random forest performed the best among the models though gradient boosting is only slightly lower. Lets choose Random forest as our final Model with accuracy of 0.9986 and kappa of 0.9983. Random forest has about `r model$finalModel$ntree` trees. 

So based on the cross validation test performance, we can estimate the accuracy (1- out sample error) on the testing dataset to be around min and max of the in sample error which is between 0.9952 to 1.0. 
So lets predict on the testing dataset and check the accuracy and kappa
```{r, cache=TRUE,echo=FALSE,message=FALSE}
pred <- predict(model, newdata=validation)
confusionMatrix(pred, validation$classe)
```
The out sample error is 0.9988 which is close to our prediction. So our Random Forest model has fitted very well for predicting the exercise quality. 

Finally lets predict the classe levels for the 20 test cases provided in the assignment using random Forest model. 
```{r, cache=TRUE,echo=FALSE,message=FALSE}
test_pred <- predict(model, newdata=testing)
sapply(test_pred, function(x) classe_levels[x])

stopCluster(cluster)
```

## Conclusion

So in conclusion, the 34 predictors with random forest model has given has a high prediction accuracy in exercise quality. With more fine tuning of model parameters and further reduction in predictors, we could speed up the training process. 
